# Visual Interpretability and Object Segmentation with Attention Maps

This project focuses on building interpretable computer vision pipelines using attention-based visual explanation methods and segmentation models. The work explores both modern deep learning techniques and alternative learning paradigms without backpropagation, supported by classical computer vision approaches.

## Key Features

- **Attention-Based Interpretability:**  
  Implementing visual explanation techniques such as **Grad-CAM**, **Attention Rollout**, and **Layer-wise Relevance Propagation (LRP)** to analyze model decision patterns.

- **Segmentation & Scene Understanding:**  
  Developing object segmentation and scene parsing pipelines for real-world driving datasets.

- **Backpropagation-Free Learning:**  
  Experimenting with training approaches inspired by **Forward-Forward Algorithm (FFA)** and comparing them with traditional deep learning methods to study robustness and interpretability.

- **Technologies:**  
  `Python`, `PyTorch`, `OpenCV`, segmentation models, attention visualization tools.

- **Datasets:**  
  Cityscapes, KITTI, Berkeley DeepDrive

- **Research Focus:**  
  Explainable AI (XAI), attention interpretability, segmentation quality, alternative training paradigms, and comparisons between classical CV and modern attention-based models.
